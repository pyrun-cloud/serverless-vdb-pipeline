{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab60a49",
   "metadata": {},
   "source": [
    "# Serverless vector-db pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660eb409",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5facb3fa",
   "metadata": {},
   "source": [
    "Import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73eb531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from vectordb.benchmarks import calculate_mult_recall\n",
    "from vectordb.serverless_vectordb import ServerlessVectorDB\n",
    "import csv\n",
    "import json\n",
    "from lithops import Storage\n",
    "from io import StringIO\n",
    "import time\n",
    "import boto3\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3c6f4",
   "metadata": {},
   "source": [
    "Inicialize serverless-vectorDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_vectordb = ServerlessVectorDB(\n",
    "        # General arguments\n",
    "        dataset = \"deep_100k\",\n",
    "        features = 96,          # dimension of vectors\n",
    "        k_search = 10,          # count of vectors returned per lambda query  \n",
    "        k_result = 10,          # count of returned vectors in the query call\n",
    "        \n",
    "        # Custom algorithm arguments\n",
    "        num_index = 32,         # the number of indices the dataset is slit into\n",
    "        k = 512,                # n_list\n",
    "        n_probe = 32,\n",
    "        query_batch_size = 8,   # number of indices accessed per lambda query\n",
    "\n",
    "        # Storage\n",
    "        storage_bucket = \"acanadilla-vectordb-datasets\",\n",
    "\n",
    "        # Runtime\n",
    "        index_mem = 10240,\n",
    "        search_map_mem = 8192,\n",
    "        search_reduce_mem = 2048\n",
    "    )\n",
    "\n",
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0caa0a2",
   "metadata": {},
   "source": [
    "## Vector Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78787c43",
   "metadata": {},
   "source": [
    "In this case, only full-dataset indexing is supported. Indexing vectors individually or in small batches is not available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a2ad7",
   "metadata": {},
   "source": [
    "Each vector of dataset needs to have the following format:\n",
    "```vector_id,vector```\n",
    "\n",
    "Where __vector_id__ is an integer, and __vector__ is a space-separated list of floats (v1 v2 v3 v4 v5 ...).\n",
    "\n",
    "__Example__: 0,-0.13469987 0.10494248 0.034127206 -0.07105535 0.051401354 0.013269722 -0.08894723 0.07330574 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9baa5",
   "metadata": {},
   "source": [
    "__Important__: the indexed dataset is located in the bucket at the path: `indexes/{dataset}/blocks/{num_index}/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a187f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = sv_vectordb.indexing(f'vectors_deep_100k.csv', 128)     # Name of file with vectors in the bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64340ac",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba9507",
   "metadata": {},
   "source": [
    "In a query, both batch and individual query are supported. However, individual queries are less efficient, so __batch querying is recommended__ - the larger the batch, the better the performance.\n",
    "\n",
    "Queries return a list of `vector_id` of size `k_reasult`. The __distance__ for each vector_id is __not returned__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6674e61a",
   "metadata": {},
   "source": [
    "The queries needs to have the following format:\n",
    "```vector```\n",
    "\n",
    "Where __vector__ is a space-separated list of floats (v1 v2 v3 v4 v5 ...).\n",
    "\n",
    "__Example__: 0.14236236 -0.06880325 -0.12708192 0.0344065 -0.061641872 0.0716707 -0.024206704 -0.026985524 -0.021729523 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a469e",
   "metadata": {},
   "source": [
    "***\n",
    "Load queries from a file located in the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65907d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES_FILE = \"queries_deep_100k.csv\"\n",
    "\n",
    "# Load csv with vectors\n",
    "storage.download_file(sv_vectordb.params.storage_bucket, QUERIES_FILE, 'queries.csv')\n",
    "with open(\"queries.csv\", \"r\") as f:\n",
    "    csv_reader_q = csv.reader(f)\n",
    "\n",
    "    query_vectors = []\n",
    "    for lines in csv_reader_q:\n",
    "        vector = lines[0].split(\" \")\n",
    "        vector = [float(value) for value in vector if value != '']\n",
    "        query_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a521fca",
   "metadata": {},
   "source": [
    "***\n",
    "Group queries in different batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35060cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_chunks(l, n): \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] \n",
    "\n",
    "BLOCK_QUERY_SIZE = 100\n",
    "\n",
    "mult_query = list(divide_chunks(query_vectors, BLOCK_QUERY_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30538ec2",
   "metadata": {},
   "source": [
    "***\n",
    "Carry out batch queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8733d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "calculated_neighbors = []\n",
    "for query in mult_query:\n",
    "            \n",
    "    if i == 1:  # Only one batch\n",
    "        break\n",
    "\n",
    "    smart_neighbors, querying_times = sv_vectordb.search(i, np.array(query))\n",
    "    calculated_neighbors.append(smart_neighbors)\n",
    "    print(f'Query block {i}: {smart_neighbors}')\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6e895",
   "metadata": {},
   "source": [
    "## Query recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac936d54",
   "metadata": {},
   "source": [
    "Estimation of the average recall for the executed queries. To calculate it, a file containing the true neighbors for each query is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8739175",
   "metadata": {},
   "source": [
    "***\n",
    "Load true neighborns from a file located in the bucket.\n",
    "\n",
    "Each line have the following format:\n",
    "```vector_ids```\n",
    "\n",
    "Where __vector_ids__ is a comma-separated list of vector_id (123,345,10,4576,...). Therefore, it is necessary to transform the data into the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18713aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_FILE = \"true_neighbours_deep_100k.csv\"     # This file only have true neighbors of first 1000 queries\n",
    "\n",
    "storage.download_file(sv_vectordb.params.storage_bucket, TRUE_FILE, 'true_neighbors.csv')\n",
    "with open(\"true_neighbors.csv\", \"r\") as f:\n",
    "    csv_reader_t = csv.reader(f)\n",
    "    \n",
    "    true = []\n",
    "    for row in csv_reader_t:\n",
    "        res_ids = [int(value) for value in row if value != '']\n",
    "        true.append(res_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8057ea",
   "metadata": {},
   "source": [
    "***\n",
    "Calcule recall of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0927371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average recall: 99.5%\n"
     ]
    }
   ],
   "source": [
    "def calculate_mean_mult(data):\n",
    "    aux = 0\n",
    "    total_len = 0\n",
    "    for x in data:\n",
    "        total_len += len(x)\n",
    "        for y in x:\n",
    "            aux += y\n",
    "            \n",
    "    return aux / total_len\n",
    "\n",
    "recalls = []\n",
    "for i, neighbors in enumerate(calculated_neighbors):\n",
    "    index = i*BLOCK_QUERY_SIZE\n",
    "    recalls.append(calculate_mult_recall(true[index:index+BLOCK_QUERY_SIZE], neighbors))\n",
    "\n",
    "recall_mean = calculate_mean_mult(recalls)\n",
    "print(f'Average recall: {recall_mean}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ceb9d",
   "metadata": {},
   "source": [
    "## Clean environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376d3d4",
   "metadata": {},
   "source": [
    "Delete indexed vectors (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c252e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f'indexes/{sv_vectordb.params.dataset}/{sv_vectordb.params.implementation}/{sv_vectordb.params.num_index}'\n",
    "keys = storage.list_keys(sv_vectordb.params.storage_bucket, prefix=prefix)\n",
    "\n",
    "response = storage.delete_objects(sv_vectordb.params.storage_bucket, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8badea04",
   "metadata": {},
   "source": [
    "Delete local temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c9ba9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: queries.csv\n",
      "Deleted: true_neighbors.csv\n"
     ]
    }
   ],
   "source": [
    "files = [Path(\"queries.csv\"), Path(\"true_neighbors.csv\")]\n",
    "\n",
    "for f in files:\n",
    "    if f.exists():\n",
    "        f.unlink()\n",
    "        print(f\"Deleted: {f}\")\n",
    "    else:\n",
    "        print(f\"File not exist: {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
